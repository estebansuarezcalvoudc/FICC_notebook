{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710bfdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../figuras/logos/logo_usc.jpg\" align=right width='80px'/>\n",
    "<br>\n",
    "\n",
    "\n",
    "<table width=\"100%\">\n",
    "<td style=\"font-size:40px;font-style:italic;text-align:right;background-color:rgba(0, 220, 170,0.7)\">\n",
    "Elements of Quantum Information\n",
    "</td></table>\n",
    "\n",
    "\n",
    "\n",
    "$ \\newcommand{\\bra}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\ket}[1]{|#1\\rangle} $\n",
    "$ \\newcommand{\\braket}[2]{\\langle #1|#2\\rangle} $\n",
    "$ \\newcommand{\\ketbra}[2]{| #1\\rangle \\langle #2|} $\n",
    "$ \\newcommand{\\tr}{{\\rm Tr}\\,} $\n",
    "$ \\newcommand{\\Tr}{{\\rm Tr}\\,} $\n",
    "$ \\newcommand{\\i}{{\\color{blue} i}} $ \n",
    "$ \\newcommand{\\Hil}{{\\cal H}} $\n",
    "$ \\newcommand{\\V}{{\\cal V}} $\n",
    "$ \\newcommand{\\Lin}{\\hbox{Lin}}$\n",
    "$ \\newcommand{\\Xn}{X^{\\! n}}$\n",
    "$ \\newcommand{\\xn}{{\\bf x}}$\n",
    "$ \\newcommand{\\bxn}{\\bar{\\bf x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3daa108",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "- [Elements of information theory](#class_info) \n",
    "    - [Shannon Entropy](#shannon)\n",
    "    - [Data Compression](#data_com)\n",
    "    - [Joint entropies](#joint_ent)\n",
    "\n",
    "        - [Conditional Entropy](#cond_ent)\n",
    "    \n",
    "        - [Mutual Information](#mut_ent)\n",
    "    \n",
    "        - [Relative Entropy](#rel_ent)\n",
    "    \n",
    " <br>\n",
    " \n",
    " - [Quantum Information](#quant_info)  \n",
    "     - [Von Neumann Entropy](#vonNeu)\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608d032",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='class_info'></a>\n",
    "\n",
    "# Elements of information theory\n",
    "[<<<](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4099a90d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For completeness, let’s review the fundamental aspects of classical information theory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176cc4f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Information** and **Probability** are dual concepts: \n",
    " a *random variable* $(X, p_X)$ produces events $x$ with probability $p_X(x)$.\n",
    "\n",
    "*information* of an event is proportional to the *uncertainty that it removes* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a6f856",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There is no uncertainty removal (no information gain) from a *sure event* $p(x)=1$.  \n",
    "\n",
    "- On the contrary, a *rare event* $p(x)\\ll 1$, increases our information a lot  \n",
    "\n",
    " \n",
    " \n",
    "Claude Shannon (*A Mathematical Theory of Communication, 1948*) : can we quantify information, $i_X(x)$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4752e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mathematical properties expected for, $i_X(x)$, must be: continuous, monotonous and additive. \n",
    "\n",
    "\n",
    " - $i_{ X}(x)\\sim 1/p_X(x)$\n",
    "\n",
    "\n",
    " - $i_X(x \\land y) = i_X(x) + i_X( y)$ for $x$ and $y$ independent\n",
    "\n",
    "\n",
    " - $i_X(x) = 0$ if $p_X(x)=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff9619",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Shannon's information measure of an event  is the **surprise function**\n",
    "\n",
    "$$\n",
    "i_X(x) =  - \\log_2 p_X(x) \\,  ~ \\hbox{(bits)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b1293",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='shannon'></a>\n",
    "## Shannon Entropy\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b> <i>(Shannon Entropy)</i>  \n",
    "<br>\n",
    "The Shannon entropy associated with a classical random variable $X = (x, p(x))$ is given by the <i>average surprise</i>\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "H(X) = -\\sum_{x} p(x) \\log p(x)\n",
    "$$  \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3901c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The (Shannon) entropy   measures  \n",
    "\n",
    "- the *average uncertainty* associated to the random variable\n",
    "<br>\n",
    "- the *average  information* that we gain by knowing events $x$ of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212cb77",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Hence $H(X)$ is an *average* over events $x$ in which \n",
    "\n",
    "- $x$ with  $p_X(x)\\approx 1$ barely contributes to $H(X)$ because it has little information, \n",
    "<br>\n",
    "- $x$ with $p_X(x)\\approx 0$ carries a lot of information but very rarely appears.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ebe85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shannon entropy has the following properties:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b>  Let $N$ be the cardinality of  $X$, then    \n",
    "<br>\n",
    "<br>    \n",
    "$$0 \\leq H(X) \\leq \\log N$$\n",
    "<br>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31842766",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "    <summary><p style=\"text-align:right ; color:black\"> >> <i>Proof</i> </p></summary>\n",
    "<br>\n",
    "\\begin{align}\n",
    " H(X) - \\log N &= \\sum_x p(x) \\log \\frac{1}{p(x)} - \\sum_x p(x) \\log N \\nonumber \\\\\n",
    "&=  \\sum_x p(x) \\log \\left(  \\frac{1}{N p(x)} \\right) \\leq  \\sum_x p(x)  \\left(  \\frac{1}{N p(x)}  - 1\\right) \\nonumber\\\\\n",
    "&=   \\sum_x  \\left(  \\frac{1}{N }  - p(x)\\right) = 0 ~~~\\Rightarrow ~~~~ \\fbox{$H(X) \\leq \\log N$} \\nonumber\n",
    "\\end{align}\n",
    "The inequality $\\log x \\leq x-1$ has been used and follows from plotting\n",
    "<img src=\"figures/logarithm.png\" width=\"30%\" style=\"margin:auto\"/>\n",
    "    \n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4385b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " The two limits correspond to particular *random variables*\n",
    " \n",
    " -  $H(X) =0$ $~~$ for the *certain* distribution (no removable uncertainty)\n",
    " \n",
    " $$p_X(x) = \\delta_{x,x_0}$$  \n",
    "\n",
    "<br>\n",
    "\n",
    " -  $H(X)= \\log N$ $~~$ for the  *even*  distribution (maximal removable average uncertainty)\n",
    " \n",
    "\n",
    "$$p_X(x) = \\frac{1}{N}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0aa71",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Suppose $(X,p_X)$ and $(Y,p_Y)$ are two random variables over the same set $X = Y = \\{x_i\\}$.\n",
    "\n",
    "Then $(Z, p_Z)$ with $Z = X = Y$ and $p_Z = \\lambda p_X + (1-\\lambda) p_Y$ is a *bona fide* random variable. \n",
    "\n",
    "We abreviate this\n",
    "random variable by $Z = \\lambda X + (1-\\lambda) Y$ by abuse of language. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d1dc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b>   \n",
    "$H(X)$ is a <i>concave</i> function of $X$. For $\\lambda \\in (0,1)$ and $X$, $Y$ two random variables\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "H\\left( \\rule{0mm}{4mm} \\lambda X + (1-\\lambda) Y)\\right) ~ \\geq ~ \\lambda H(X) + (1-\\lambda) H(Y)\n",
    "$$\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ced4a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Concavity implies that there is **more average removable uncertainty** in $p_Z =  \\lambda p_X(x) + (1-\\lambda)p_Y(y)$, than  the **sum of removable uncertainties** in  each random variable individually.\n",
    "\n",
    "The [proof of concavity](#proof_of_concavity) will follow later from the positivity of the relative entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be7aa92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\", text-align:center>\n",
    "<b>Example:</b>\n",
    "\n",
    "The entropy of the binary (or Bernoulli) distribution $ \\{ (a,b), (p, 1-p) \\} $ is  \n",
    "<br>\n",
    "$$\n",
    "H(p) = - p\\log p - (1-p)\\log (1-p)\n",
    "$$\n",
    "<br>\n",
    "We can see in the figure that it is indeed a concave function\n",
    "<br>\n",
    "<br>    \n",
    "    \n",
    "<img src=\"figures/binaryentropy.png\" width=\"30%\" style=\"margin:auto\"/>\n",
    "\n",
    "The case $p = 1/2$ marks the maximum of the entropy. At this value, on average, knowing the outcome $a$ or $b$ of a trial removes the greatest amount of uncertainty.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673a877",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='data_comp'></a>\n",
    "## Data Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123d137",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Another interpretation of $H(X)$ comes from the amount of resources needed to transmit messages composed of *letters* from the random variable $X$. This is the content of Shannon’s first theorem.\n",
    "\n",
    "To transmit a message through a channel, a process of compressed encoding is used. Typically, this is done in bits $\\{0,1\\}$, but using dits $\\{0,...,D-1\\}$ is also possible.\n",
    "\n",
    "In fact, associated with encoding in dits, we define $H_D(X) = -\\sum_{x} p(x) \\log_D p(x)$. When $D$ is not specified, we assume $D=2$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d850a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\", text-align:center>\n",
    "<b>Example:</b>\n",
    "encode an <i>alphabet</i> made of 4 letters\n",
    "$x=\\{A,B,C,D\\}$ which appear with probabilities $p_X(x)$ using bit-strings of length $l(x)$where\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "$$\n",
    "p(A) = \\frac{1}{2}~,~~ p(B)=\\frac{1}{4}~,~~ p(C) =p(D) = \\frac{1}{8}.\n",
    "$$\n",
    "<br>\n",
    "    \n",
    "- <u>strategy 1</u>. Encode uniformly with 2 bits for 4 letters \n",
    "\n",
    "$$\n",
    "A= 00~,~~ B=01~,~~ C=10~,~~ D=11 \\, .\n",
    "$$     \n",
    "The average <b>codeword length per letter</b> is\n",
    "    \n",
    "$$ \n",
    "L = \\sum_{x}^4 p(x) l(x) = \\frac{1}{2}\\times 2 + \\frac{1}{4}\\times 2 +2\\times \\frac{1}{8} \\times 2 = 2$$\n",
    "    \n",
    "\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd8fe3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\", text-align:center>\n",
    "    \n",
    "    \n",
    "- <u>strategy 2</u>.  In order to get higher compression it makes sense to adscribe less bits to the letters that appear more frequently\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "A= 0~,~~ B=10~,~~ C=110~,~~ D=111 \\, .\n",
    "$$\n",
    "<br>  Average codeword length (number of bits)  per letter is lower now than in the uniform case\n",
    "<br>    \n",
    "<br>    \n",
    "$$L =   \\frac{1}{2}\\times 1 + \\frac{1}{4}\\times 2 +2\\times \\frac{1}{8} \\times 3 = \\frac{7}{4} < 2$$\n",
    "<br>\n",
    "    \n",
    "<b> Remarkably</b>, in the second case, the average length per letter <u>coincides with Shannon</u> entropy of the random variable\n",
    "<br>    \n",
    "$$\n",
    "H(X) = -\\frac{1}{2}\\log(1/2) - \\frac{1}{4}\\log(1/4) - 2\\times \\frac{1}{8}\\log(1/8) = \\frac{7}{4}\\, .\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dae6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shannon optimal  compression theorem\n",
    "\n",
    "What is the **optimal coding** for a random variable $(X, p_X)$ with $N$ letters $a_1,..., a_N$?\n",
    "\n",
    "\n",
    "Shannon's brilliant idea is to shift the focus from coding individual letters to coding full sequences\n",
    "<br>\n",
    "<br>\n",
    "$$\\xn = x_1 x_2.... x_n$$\n",
    "<br>\n",
    "of $n$ letters pulled from $X$  independently from one another and *iid* (identically distributed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482e5d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- strings $\\xn$ are elements of a new random variable $\\Xn$\n",
    "<br>\n",
    "\n",
    "- therefore the probability to draw a particular string is\n",
    "\n",
    "$$\n",
    "p_{{\\Xn}}(\\xn) = p_X(x_1) p_X(x_2) ...p_X(x_n) = \\prod_{i=1}^n p_X(x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacb0c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- in the string $\\xn$ the letter $a_i$ appears  $N(a_i | \\xn)$ times. \n",
    "\n",
    "$$\n",
    "\\xn \\sim \\underbrace{a_1\\cdots a_1}_{N(a_1|\\xn)}\\underbrace{a_2\\cdots a_2}_{N(a_2|\\xn)} \\cdots \\underbrace{a_N\\cdots a_N}_{N(a_N|\\xn)}  \n",
    "$$\n",
    "<br>\n",
    "Therefore we can rewrite\n",
    "\n",
    "$$\n",
    "p_{{\\Xn}}(\\xn)= \\prod_{i=1}^n p_X(x_i) = \\prod_{i=1}^N p_X(a_i)^{N(a_i,\\xn)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd61ae2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The *amazing fact* is that in the limit of large $n\\gg 1$, **not every possible sequence will appear**, but just those in a *very restricted set*, called the **typical set** $T(n,\\epsilon)$. \n",
    "\n",
    "Essentially, $ \\bxn \\in T(n,\\epsilon)$ is composed mostly of those sequences for which \n",
    "\n",
    "$$ \\left|\\frac{N(a_i|  \\bxn)}{n}  - p_X(a_i) \\right| \\approx  \\epsilon \\ll 1$$\n",
    "\n",
    "with equality in the limit of long sequences $n\\to \\infty$. $~$\n",
    "\n",
    "Let us try to be more precise. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3bc98d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define the  **typical set** $T(n,\\epsilon)$ by computing the **statistical average surprise**  of a string $\\xn$\n",
    "\n",
    "\\begin{align}\n",
    "E(-\\log_X p) &= \\sum_{i=1}^N -\\frac{N(a_i|\\Xn)}{n} \\, \\log  p_X(a_i) \\\\\n",
    "&=  -\\frac{1}{n} \\sum_{i=1}^N \\log\\left( p_X(a_i)^{N(a_i|\\Xn)} \\right) \\\\\n",
    "&= -\\frac{1}{n} \\log\\left( \\prod_{i=1}^N p_X(a_i)^{N(a_i|\\Xn)}\\right) \\\\\n",
    "&=  -\\frac{1}{n} \\log \\, p_{{\\Xn}}(\\xn)  = \\frac{1}{n} i_{{\\Xn}}(\\xn) \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "If we compare this to the Shannon entropy of the random variable\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_i p_X(a_i) \\log p_X(a_i)\n",
    "$$\n",
    "we see clear the motivation to define \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4187bc4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b>  \n",
    "<br>\n",
    "The typical set $T(n,\\epsilon)$ is composed of all the strings $\\bar\\xn$ that fulfill\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    " \\left\\vert\\frac{1}{n} i_{\\Xn}(\\bar\\xn) - H(X) \\right\\vert\\leq \\epsilon \\label{eqgrn}\n",
    "$$\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "or, in other words\n",
    "$$\n",
    " n\\big( H(X) - \\epsilon \\big) ~\\leq ~  i_{\\Xn}\\big(\\bar\\xn) ~\\leq ~  n\\big( H(X) +  \\epsilon \\big)    \\label{seqtip}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3bc01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b><i>$~$(Weak Law of Large Numbers</i>)  \n",
    "<br>\n",
    "Given any $\\epsilon, \\delta$, there exists a (large enough) value of $n$ for which the probability of $\\xn$ pertaining to $T(n,\\epsilon)$ is above $(1-\\delta)$\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "p_{\\Xn}(\\xn) \\in T(n,\\epsilon) \\geq (1-\\delta)\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Hence, we can take $\\delta, \\epsilon\\to 0$ in the large $n$ limit, where only typical elements of $\\Xn$ will be generated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d09144",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define the **limiting typical set**  $~T = \\lim_{\\epsilon\\to 0} \\lim_{n\\to\\infty} T(n,\\epsilon)$,\n",
    "with cardinality $T$. The previous results can be summarized as follows \n",
    "\n",
    "\n",
    "- if $\\xn \\notin T(n,\\epsilon)$, the probability of appearance  will uniformly tend to zero $p_{\\Xn} (\\xn) \\stackrel{n\\to \\infty}{\\longrightarrow} 0$\n",
    "<br>\n",
    "\n",
    "- if $\\xn = \\bxn \\in T(n,\\epsilon)$ the probability of appearance  will uniformly tend to  $p_{\\Xn} (\\bxn) \\stackrel{n\\to\\infty}{\\longrightarrow} 1/|T|$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fce320",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exerise 1.1</b>\n",
    "<br>\n",
    "\n",
    "Make a code that illustrates the way the typical set $T(n,\\epsilon)$ takes over the set of generated $n-$letter strings as $n$ grows. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a8a89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Depending on the distribution $p_X$, this can yield  <u>a dramatic reduction in the amount of possible sequences</u>. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb91f7b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For large $n$, the different $\\bxn$ will be just reorderings of a canonical sequence with $N(a_i|\\bar \\xn) = n p_X(a_i)$. Hence   \n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "N_{typical} = |T| = [\\bxn ]\\approx  \\frac{n!}{\\prod_{i=1}^N N(a_i | \\bxn)!} &= \\frac{n!}{\\prod_{i=1}^N (n p_X(a_i))!} = 2^{\\log n! - \\sum_{i=1}^N \\log (n p_X(a_i))!} \\\\ \n",
    "\\rule{0mm}{10mm}&\\sim  2^{n\\log_2 n - {n/\\log_2} e \\, -\\,  \\sum_i \\left( np_X(a_i) \\log n p_X(a_i)- n p_X(a_i) \\log_2 e \\right)}  \\\\ \n",
    "\\rule{0mm}{10mm}\n",
    "& =  2^{n H(X)}   \n",
    "\\rule{0mm}{8mm}   \\,  \\nonumber\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c309e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Its size is *exponentially smaller* than that of possible sequences\n",
    "\n",
    "$$\n",
    "\\frac{N_{typical} }{N_{possible}} =  \\frac{[\\bxn ]}{N^n} = \\frac{2^{n H_X}}{  2^{n\\log N}} = 2^{-n(\\log N- H(X))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dad23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\", text-align:center>\n",
    "<p style=\"text-align: left \">  \n",
    "<b> Summary </b>\n",
    "In the limit $n \\to \\infty$, the following three things happen\n",
    "\n",
    "- The number of sequences in the typical set is $|T| ~ \\stackrel{n\\to\\infty}{\\longrightarrow} ~ 2^{n H(X)}$.\n",
    "\n",
    "- The sequences in the typical set are equiprobable. Necessarily:\n",
    "\n",
    "$$\n",
    "  p(\\bxn_0) \\stackrel{n\\to\\infty}{\\longrightarrow} |T|^{-1} =  2^{- nH(X)}\n",
    "  $$\n",
    "\n",
    "- Any sequence <b>not</b> in the typical set has, in the limit, zero probability of occurring.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87d5d1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The previous observations allow us to establish a message encoding procedure: it only makes sense to encode the $|T| = 2^{nH(X)}$ typical messages that are likely to appear.\n",
    "\n",
    "This can be done using strings with $n H(X)$ bits per message, or $H(X)$ bits per letter.\n",
    "\n",
    "If $X$ is a uniformly distributed alphabet, then $H(X) = \\log N$, and it is not possible to compress it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c53353",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Compression strategy**: \n",
    "\n",
    "- code by labelling only typical sequences $\\xn$ using bits.  \n",
    "<br>\n",
    "\n",
    "- Being there $|T| = 2^{nH(X)}  \\Rightarrow$ sequences,  coding them needs only  $nH(X)$ bits.\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f1b7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b> <i>(Shannon Noiseless Coding Theorem, (1984))</i>  \n",
    "<br>\n",
    "<br>    \n",
    "Let $X = \\{a_i, p(a_i)\\}$ be a stochastic source of $N$ letters. Words of length $n \\to \\infty$ admit a  <b>lossless encoding</b> thay makes use, on average, of $n H(X)$ bits\n",
    "<br>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c1fa8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\", text-align:center>\n",
    "<p style=\"text-align: left \">  \n",
    "    <b>Note:</b> the quantity $~ \\log N - H(X) ~$ quantifies the <i>compressibility</i> of a random source $X$   \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e8ad7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By abuse of language, we may say that the optimal lossless encoding makes use, on average, of $H(X)$ *bits per letter* \n",
    "<br>\n",
    "\n",
    "- The equipartitioned alphabet, $p_1 = p_2 = ...=  1/N$,  has maximal entropy $H(X) = \\log N$, and each letter removes in average the highest amount of information. \n",
    "<br>\n",
    "\n",
    "- Therefore, it **admits no compression**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f996810",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='joint_ent'></a>\n",
    "## Joint Entropies\n",
    "[<<<](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f525326",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a communication processes we alway deal with (at least) *two random variables*: the emission $X = (x, p(x))$ and the reception $Y = (y, p(y))$. \n",
    "\n",
    "The joint occurrence of a pair of values $x$ and $y$ at the ends of the channel defines a new random variable $XY = \\{xy, p(x,y)\\}$, where $p(x,y)$ is the *joint probability*.\n",
    "\n",
    "As with any random variable, we can also associate an entropy to it:\n",
    "\n",
    "$$\n",
    "H(X,Y) = - \\sum_{xy} p(x,y) \\log p(x,y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa7037",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A very important property of the Shannon entropy of a joint probability is called: **subadditivity**. \n",
    "<a id='subbaditivity'></a>\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b> <i>(subadditivity)</i>:  \n",
    " Given two random variables $X$ and $Y$:\n",
    "<br>\n",
    "<br>    \n",
    "$$ H(X,Y) \\leq H(X) + H(Y) $$\n",
    "<br>    \n",
    "with equality holding if there is no correlation between the two variables.\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da8fe8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- a [proof of the subadditivity](#proof_of_subaditivity) will provided below based on the positivity of the Relative Entropy. \n",
    "<br>\n",
    "<br>\n",
    "- the subaditivity property saturates if $Y$ and $Y$ are independent random variables, $\\Rightarrow p(x,y) = p(x) p(y)$ \n",
    "<br>\n",
    "<br>\n",
    "$$ H(X,Y) = H(X) + H(Y) $$ \n",
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<br>    \n",
    "<br>\n",
    "\\begin{align}\n",
    "H(X,Y) &=  - \\sum_{xy} p(x)p(y) (\\log p(x) + \\log p(y)) =   - \\sum_{x} p(x)\\log p(x)\\sum_y p(y)  +\\sum_x p(x)\\sum_y p(y) \\log p(y)   \\\\  \\rule{0mm}{4mm}\n",
    "& =   - \\sum_{x} p(x)\\log p(x) - \\sum_y p(y) \\log p(y) = H(X) + H(Y)\n",
    "\\end{align}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a81cf3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='cond_ent'></a>\n",
    "### Conditional Entropy\n",
    "\n",
    "The random variable  $X|y = \\{x, p(x|y)\\}$ is the mathematical tool that **characterizes noise** in a communication channel.  \n",
    "\n",
    "Specifically, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a25691",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Q_{xy} = p(x|y)$ is the *conditional probability* that $x$ happens knowing that $y$ has occurred\n",
    "<br>\n",
    "\n",
    "- in *communication contexts*, it is the probability that,  $x$ is understood at  one end provided  $y$ was sent at other \n",
    "<br>\n",
    "\n",
    "- A **noiseless channel** would satisfy $Q_{xy} = \\delta_{xy}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b56e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "As with any random variable, we can assign a Shannon entropy\n",
    "\n",
    "$$\n",
    "H(X|y) = -\\sum_x p(x|y)\\log(p(x|y)).\n",
    "$$\n",
    "\n",
    "Notice that $y$ is a fixed message being sent. We can now average $H(X|y)$ over all possible messages and obtain the **conditional entropy**:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "H(X|Y) = \\sum_y p(y)H(X|y) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6251b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Lemma:</b> The conditional entropy equals \n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "H(X|Y) =  H(X,Y) - H(Y)\n",
    "$$\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align: right\"> >> Proof </p></summary>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum_y p(y)H(X|y) &=& -\\sum_{yx} p(y)p(x|y)\\log(p(x|y)) \\\\\n",
    "   &=& -\\sum_{yx} p(x,y)\\log\\left(\\frac{p(x,y)}{p(y)}\\right) \\\\\n",
    "   &=& -\\sum_{yx} p(x,y)\\log(p(x,y)) + \\sum_{yx} p(x,y)\\log(p(y)) \\\\\n",
    "   &=& H(X,Y) - H(Y)\n",
    "\\end{eqnarray}\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf0499",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$H(X,Y)$ reflects the *residual uncertainty* that can be removed by learning $X$ (*residual information*) after having received many messages $y \\in Y$.\n",
    "\n",
    "- If $X$ and $Y$ are **independent** variables, then $H(X|Y) = H(X)+ H(Y) - H(Y) = H(X)$, meaning that knowing $Y$ does not reduce the uncertainty removed by learning $X$  \n",
    "<br>\n",
    "\n",
    "- If $X = Y$, are **correlated** then $H(X|Y) = 0$, knowing $Y$ removes all the uncertainty in $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d4f38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='mut_inf'></a>\n",
    "### Mutual Information\n",
    "\n",
    "\n",
    " - $H(X)$ is a measure of the amount of information $X$—that is, the average  amount of uncertainty that is removed when an event $x$ is known.\n",
    "<br>\n",
    "\n",
    " - $H(X|Y)$ is a measure of the information in $X$ after  knowing $Y$\n",
    "\n",
    "Necessarily, $H(X|Y) < H(X)$, since one cannot remove more uncertainty than the one that originally exists in $X$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3c2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The difference is therefore positive, $H(X) - H(X|Y) > 0$, and it quantifies the amount of information *gained* by the receiver. This is called *mutual information*:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b> The <b>mutual information</b> of two random variables $I(X,Y)$ equals\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "I(X,Y) = H(X) + H(Y) - H(X,Y) \n",
    "$$\n",
    "</div>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc418344",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The mutual information is non-negative $I \\geq 0$. This trivially follows from the [subadditivity property](#subbaditivity) of the Shannon entropy of a joint random variable.\n",
    "<br>\n",
    "<br>\n",
    "- We see that the gain in information is a symmetric quantity under $X \\leftrightarrow Y$ \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) \\, .\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb34c7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, $I(X,Y)$ is a measure of the *correlations* between the two variables.\n",
    "\n",
    "- If there is no correlation, then measuring $Y$ provides no information, and the mutual information is zero:  $I(X,Y) = 0$.  \n",
    "<br>\n",
    "\n",
    "- If they are perfectly correlated, $I(X,Y) = H(X)$, then there is a perfect correlation between emission and reception, and the transmitted information reaches its maximum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb3289",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='rel_ent'></a>\n",
    "\n",
    "### Relative Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2ad38",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Relative entropy is a measure of the *distance* between the probability distributions $p(x)$ and $q(x)$, defined over the same sample space $x \\in X$  \n",
    "<br>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(p|| q) &=& \\sum_x p(x) \\big( \\log p(x)- \\log q(x) \\big)  \\\\\n",
    "&=&  - H(X) -  \\sum_x p(x) \\log q(x)\\, .\n",
    "\\end{eqnarray}\n",
    "\n",
    "This distance is sometimes called the *Kullback-Leibler divergence*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32a13e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b>  <i>(Gibbs' Inequality)</i>:  \n",
    "<br>\n",
    "Relative entropy is <i>non-negative</i>. That is, for two arbitrary distributions $p(x)$ and $q(x)$, we have:\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "H(p|| q) ~=~  \\sum_x p(x)  \\log p(x) -  \\sum_x p(x)  \\log q(x)  ~\\geq~ 0\n",
    "$$\n",
    "<br>\n",
    "and the inequality is saturated if and only if $p(x) = q(x)$, that is, the distributions are identical.\n",
    "</p>\n",
    "</div>    \n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<br>\n",
    "In general, we have:\n",
    "\\begin{eqnarray}\n",
    "\\sum_x p(x)  \\log \\frac{p(x)}{q(x)} & ~= ~&   \\sum_x p(x) \\log\\frac{p(x)}{q(x)}    ~= ~ - \\sum_x p(x)   \\log\\frac{q(x)}{p(x)} \\nonumber\\\\\n",
    "&~\\geq ~& -  \\log  \\sum_x p(x)  \\frac{q(x)}{p(x)}   ~= ~ -  \\log  \\sum_x q(x)   \\nonumber\\\\\n",
    "&=&  -\\log 1 =  0  \\nonumber\n",
    "\\end{eqnarray} \n",
    "<br>\n",
    "where the inequality follows from the fact that $- \\log x$ is a convex function, in any base.  \n",
    "For equality, $p(x) = q(x)$ is a sufficient condition. To see that it is also necessary, consult references.\n",
    "<br>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f5c48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By considering several particular cases, we can now prove some of the statements made earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc93f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Shannon entropy is bounded by the cardinality, $N$, of the sample space**\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "H(X) \\leq \\log N\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<br>\n",
    "<br>\n",
    "Just take  \n",
    "$q_X(x) = 1/N$. Then  \n",
    "$H(p_X || q_X) = - H(X) + \\log N \\geq 0$.\n",
    "<br>\n",
    "<br>\n",
    "The inequality is saturated when $X$ is also uniformly distributed, i.e., $p_X(x) = q_X(x) = 1/N ~\\Rightarrow~ H(p_X || q_X) = 0$.\n",
    "<br>\n",
    "<br>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b6cd73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='proof_of_subaditivity'></a>\n",
    "- **Shannon entropy is subadditive**\n",
    "\n",
    "$$\n",
    "H(X,Y) \\leq H(X) + H(Y)\n",
    "$$\n",
    "\n",
    "This proves also that the Mutual Information is positive.\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "I(X,Y) = H(X) + H(Y) - H(X,Y) \\geq 0\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<br>\n",
    "<br>  \n",
    "Now consider the joint random variable $XY \\to \\{(x,y)\\}$, where $p_X$ is the joint distribution $p_{XY}$ and $q_X$ is the factorized distribution $p_X p_Y$, with  \n",
    "$p_X(x) = \\sum_y p_{XY}(x,y)$ and $p_Y(y) = \\sum_x p_{XY}(x,y)$ being the marginal distributions.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(p_{X,Y}(x, y) || p_X(x) p_Y(y)) \n",
    "&=& - H(X,Y) - \\sum_{x,y} p_{XY}(x,y) \\log p_X(x) - \\sum_{x,y} p_{XY}(x,y) \\log p_Y(y) \\nonumber\\\\\n",
    "&=& - H(X,Y) - \\sum_x p_X(x) \\log p_X(x) - \\sum_y p_Y(y) \\log p_Y(y) \\nonumber\\\\\n",
    "&=& - H(X,Y) + H(X) + H(Y) \\\\\n",
    "&=& I(X,Y) \\geq 0 \\rule{0mm}{8mm}\n",
    "\\end{eqnarray}\n",
    "<br>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce1fe11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This inequality proves that mutual information is non-negative. Equivalently\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "H(X,Y) \\leq H(X) + H(Y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05569bc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='proof_of_concavity'></a>\n",
    "\n",
    "- **Shannon entropy is a concave function**\n",
    "<br>\n",
    "<br>\n",
    "Let $p_Z = \\lambda p_X + (1-\\lambda) p_Y$ be the $\\lambda-$averaged distribution of $p_X$ and $p_Y$ over the same set $X = Y = Z = \\{x\\}$. The concavity property states that, for any $\\lambda$ \n",
    "\n",
    "$$\n",
    "H(p_Z) \\geq \\lambda H(p_X) + (1-\\lambda) H(p_Y)\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right\"> >><i>Proof</i> </p></summary>\n",
    "<br>\n",
    "<br>  \n",
    "From the definition \n",
    "\\begin{align}\n",
    "H(p_X||p_Z) &= - H(p_X) - \\sum_x p_X(x) \\log p_Z(x) \\\\\n",
    "H(p_Y||p_Z) &= - H(p_Y) - \\sum_x p_Y(x) \\log p_Z(x) \\\\\n",
    "\\end{align}\n",
    "From the positivity of the relative entropy we have that, for $0\\leq \\lambda \\leq 1$\n",
    "\n",
    "\\begin{align}\n",
    " 0 & \\leq \\lambda H(p_X||p_Z) + (1-\\lambda)H(p_Y||p_Z)   \\\\ \\rule{0mm}{8mm}\n",
    " & =  -\\lambda H(p_X) - (1-\\lambda) H(p_Y) - \\sum_x (\\lambda p_X(x) + (1-\\lambda)p_Y(x))\\log p_Z(x) \\\\  \\rule{0mm}{4mm}\n",
    " & = -\\lambda H(p_X) - (1-\\lambda) H(p_Y) + H(p_Z)\n",
    "\\end{align}\n",
    "\n",
    "From here it follows the concavity property\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75c76b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='quant_info'></a>\n",
    "\n",
    "# Quantum Information\n",
    "[<<<](#top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eff98b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Suppose a classical random source generates letters from an alphabet $X = \\{x_a, p_a\\}$ with probability $p_a = p(x_a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d5bb9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The *prior uncertainty* (information) is given by the Shannon entropy:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{a=1}^r p_a \\log p_a\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f0cda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To transmit a message using a *quantum channel*, we prepare states $x_i \\to \\ket{\\psi_i}$ with $i$ devices and send them successively.\n",
    "\n",
    "In this way, we have created a *quantum signal source*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b6310",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From the receiver's point of view, this is an incoherent statistical mixture of states $X = \\{\\ket{\\psi_a}, p_a\\}$, which are received with probability $p_a = p(\\ket{\\psi_a})$.\n",
    "\n",
    "To decode the message, the receiver must guess which states compose the received state by performing measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b483de9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The <b>density operator</b> is the mathematical object that characterizes the statistical mixture that is received  \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\rho = \\sum_{a} p_a \\ket{\\psi_a}\\bra{\\psi_a}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f67f85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that:\n",
    "\n",
    "- We have not required $\\ket{\\psi_a}$ to be a set of orthogonal vectors. In general, they will not be.\n",
    "<br>\n",
    "<br>\n",
    "- The number of vectors and letters $a = 1, 2, \\dots$ may be greater or smaller than the dimension of the Hilbert space of the quantum system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4e360",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\rho$ is Hermitian, we can always write it in its *spectral representation*:\n",
    "\n",
    "$$\n",
    "\\rho = \\sum_{i=1}^N \\lambda_i \\ket{\\lambda_i}\\bra{\\lambda_i}\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ are the eigenvalues and $\\ket{\\lambda_i}$ are the corresponding eigenvectors, which form an orthonormal basis: $\\braket{\\lambda_i}{\\lambda_j} = \\delta_{ij}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20903b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This representation refers to a *hypothetical device* associated with projective measurements $\\{M_i = \\ketbra{\\lambda_i}{\\lambda_i}\\}$.\n",
    "\n",
    "We refer to the quantum random variable $\\hat C = \\{\\ket{\\lambda_i}, \\lambda_i\\}$ as the *canonical ensemble*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977ae2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='vonNeu'></a>\n",
    "\n",
    "## Von Neumann Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de20cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The described procedure presents us with two ensembles:\n",
    "\n",
    "- the *original* one, associated with the preparation: $X = \\{x_a, p_a\\} \\to \\{\\ket{\\psi_a}, p_a\\}$\n",
    "<br>\n",
    "<br>\n",
    "- the *canonical* one, associated with the diagonalization of $\\rho$: $C = \\{\\ket{\\lambda_i}, \\lambda_i\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70240b49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each ensemble has an associated Shannon entropy:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(X) &=& -\\sum_a p_a \\log p_a \\\\\n",
    "H(C) &=& -\\sum_{i=1}^N \\lambda_i \\log \\lambda_i\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e4215",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The key to the second expression is that, by definition, it is equivalent to the following:  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "H(C) = -\\Tr (\\rho \\log \\rho)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "The advantage of writing it this way is that it is *basis-independent*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d9554",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b> <i>von Neumann entropy</i>  \n",
    "<br>    \n",
    "$$\n",
    "S(\\rho) = -\\Tr (\\rho\\log \\rho)\n",
    "$$  \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb4326",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that, written in this way, $S(\\rho)$:\n",
    "\n",
    "- does not refer to *any specific* basis of states.  \n",
    "<br>\n",
    "<br>\n",
    "- is uniquely determined for each state $\\rho$  \n",
    "<br>\n",
    "<br>\n",
    "- also does not depend on the preparation procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266f4d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In short: *we can assign a von Neumann entropy to any density operator* $\\rho$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5924a4d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise:</b>  \n",
    "Write a function `S_entropy(rho)` that returns the von Neumann entropy associated with a state $\\rho$,  \n",
    "expressed as a matrix in the canonical basis.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac593849",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of the von Neumann Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375bb4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Bounds**:  \n",
    "<br>Let $N$ be the <u>dimension of $\\Hil$</u>. The von Neumann entropy is bounded by  \n",
    "<br>\n",
    "<br>$$0 \\leq S(\\rho) \\leq \\log N$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f1785",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- In a <i>pure state</i>, the von Neumann entropy is zero  \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "S(\\rho) = 0 ~~~\\Longleftrightarrow ~~~\\rho^2 = \\rho\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275dfa1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In a <i>maximally mixed state</i>, the entropy of the state is maximal  \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "S(\\rho) = \\log N ~~~\\Longleftrightarrow ~~~\\rho = \\frac{1}{N} I\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348cacb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Concavity**:  \n",
    "<br>\n",
    "\n",
    "$S(\\rho)$ is a concave function of its argument $\\rho$. For any straight line interpolating between $\\rho_1$ and $\\rho_2$:  \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "S\\left(\\rule{0mm}{4mm}\\lambda \\rho_1 + (1-\\lambda) \\rho_2 \\right) \\geq \\lambda S(\\rho_1) + (1-\\lambda) S(\\rho_2)\n",
    "$$\n",
    "<br>\n",
    "where $\\lambda \\in (0,1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02575121",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b>  \n",
    "The concavity of $S$ generalizes to linear combinations. Let $\\rho = \\sum_{i=1}^r p_i \\rho_i$, where $~\\sum_{i=1}^r p_i = 1$  \n",
    "<br>    \n",
    "<br>    \n",
    "$$\n",
    "S(\\rho) \\geq \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9709d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The proof will be given later by means of the subadditivity property.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f299ed4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Invariance**:  \n",
    "<br> The von Neumann entropy is invariant under *unitary transformations*:\n",
    "\n",
    "$$ \n",
    "S(\\rho) = S(U^\\dagger \\rho U)\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb02b95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, this implies that the von Neumann entropy of an isolated system is constant in time  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "S(\\rho(t)) = S(U(t)\\rho(0) U(t)^\\dagger) = S(\\rho(0))\n",
    "$$\n",
    "\n",
    "That is,\n",
    "\n",
    "$$\n",
    "\\frac{dS(t)}{dt} = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cc29e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Conversely, it can be shown that if $dS(t)/dt \\neq 0$, then:\n",
    "\n",
    "- the system is open  \n",
    "<br>\n",
    "<br>\n",
    "- $ \\displaystyle \\frac{dS(t)}{dt} > 0 ~~ $ entropy can only increase\n",
    "\n",
    "In this case, we speak of *incoherent evolution* or *decoherence*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de64a64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='entrop_rel'></a>\n",
    "## Relative Entropy\n",
    "\n",
    "We define the relative entropy by formal analogy with the classical case. Let $\\rho$ and $\\sigma$ be two quantum states. The relative entropy is a measure of distance that vanishes when they are equal:\n",
    "\n",
    "$$\n",
    "S(\\rho \\| \\sigma) = \\Tr \\rho(\\log\\rho - \\log \\sigma)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Gibbs' inequality for $H(X\\|Y)$ has a parallel result for $S(\\rho \\| \\sigma)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa7e1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem</b> <i>(Klein's inequality)</i>  \n",
    "<br>\n",
    "Relative entropy is non-negative:  \n",
    "$$\n",
    "S(\\rho \\| \\sigma) \\geq 0\n",
    "$$  \n",
    "<br>\n",
    "and it vanishes if and only if $\\rho = \\sigma$.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb01cc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ent_for_med'></a>\n",
    "## Preparation and Measurement Entropies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ffde5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Preparation Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a459c",
   "metadata": {},
   "source": [
    "There are infinitely many ensembles  \n",
    "$X = \\{\\ket{\\psi_a}, p_a\\},\\quad \\tilde X = \\{\\ket{\\tilde \\psi_i}, \\tilde p_i\\}, \\dots$  \n",
    "that are described by the same density operator:\n",
    "\n",
    "$$\n",
    "\\rho ~=~ \\sum_{a=1}^r p_a \\ket{\\psi_a}\\bra{\\psi_a} ~=~ \\sum_{i=1}^s \\tilde p_i \\ket{\\tilde\\psi_i}\\bra{\\tilde\\psi_i} ~=~ \\dots\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19769b57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each ensemble has an associated Shannon entropy $H(X), H(\\tilde X), \\dots$ which may differ.  \n",
    "<br>\n",
    "\n",
    "- However, the von Neumann entropy $S(\\rho)$ is the same for all of them because it depends only on $\\rho$.\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3ffbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "    <b>Definition:</b> <i>(Preparation entropy)</i>  \n",
    "<br>    \n",
    "For each ensemble $X = \\{\\ket{\\psi_a}, p_a\\}$ that prepares a state  \n",
    "$\\rho = \\sum_a p_a\\ket{\\psi_a}\\bra{\\psi_a}$,  \n",
    "we define the <i>preparation entropy</i> as the difference  \n",
    "<br>  \n",
    "<br>    \n",
    "$$\\Delta(X,\\rho) = H(X) - S(\\rho)$$\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb583d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b> The preparation entropy is non-negative, $\\Delta(X,\\rho) \\geq 0$, that is:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{eqnarray}\n",
    "S(\\rho ) ~~\\leq ~~ H(X)\n",
    "\\\\\n",
    "\\end{eqnarray}\n",
    "<br>\n",
    "The inequality is saturated for a preparation $X$ in which the states $\\{\\ket{\\psi_a}\\}$ are orthogonal.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f95d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The proof is lengthy and will not be presented here.  \n",
    "The result is plausible because $H(X) \\leq \\log(r)$, where $r$ is the number of *letters* in the ensemble $\\ket{\\psi_a}, \\, a = 1,\\dots,r$, which is unbounded, while $S(\\rho)\\leq \\log N$ is bounded by the dimension of the Hilbert space $\\Hil$.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- On the other hand, if we require the states to be orthogonal, then $r \\leq N$. We can complete them to form a basis $\\{\\ket{\\psi_a}\\}, \\, a = 1,\\dots,N$. Since $S(\\rho)$ is invariant under unitary transformations, it equals the expression written in the eigenbasis $\\{\\ket{\\lambda_a}\\}$, that is, $H$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c40550",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the source states $X = \\{\\ket{\\psi_a}, p_a\\}$ are not orthogonal, then $S < H$. But the  \n",
    "$\\{\\ket{\\psi_a}\\}$ cannot be distinguished $\\Rightarrow$ there is no observable that allows for full recovery of the information encoded in the classical message.  \n",
    "<br>\n",
    "<br>\n",
    "$\\Rightarrow \\rho$ transmits less information through the quantum channel than the amount contained in the original classical message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9f650",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Example 1:</b> Orthogonal states.  \n",
    "<br>\n",
    "<br>\n",
    "Suppose Alice has a random source of orthogonal states  \n",
    "<br>\n",
    "<br>    \n",
    "$$X = \\{ \\ket{\\psi_i}, p_i\\} = \\{ (\\ket{0}, p_0= 1/4), (\\ket{1}, p_1 = 3/4)\\}$$  \n",
    "<br>\n",
    "Bob describes the system using the density matrix  \n",
    "$$\n",
    "\\rho = p_0\\ket{0}\\bra{0} + p_1\\ket{1}\\bra{1} = \\begin{bmatrix} p_0 & 0 \\\\ 0 & p_1 \\end{bmatrix}\n",
    "$$  \n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497c227",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "and the associated Shannon entropy will be  \n",
    "<br>\n",
    "\\begin{eqnarray}\n",
    "S(\\rho) &=&  -\\Tr \\rho\\log \\rho = - \\Tr \\left( \\begin{bmatrix} p_0 & 0 \\\\ 0 & p_1 \\end{bmatrix}  \\begin{bmatrix} \\log p_0 & 0 \\\\ 0 & \\log p_1 \\end{bmatrix} \\right) \\nonumber\\\\\n",
    "&=& \\rule{0mm}{5mm}\n",
    "-p_0\\log p_0 - p_1 \\log p_1 = H(p_0,p_1) \\nonumber\n",
    "\\end{eqnarray}\n",
    "<br>\n",
    "Therefore, for orthogonal states, the Von Neumann and Shannon entropies are equal  \n",
    "<br>    \n",
    "<br>    \n",
    "\\begin{eqnarray}\n",
    "S(\\rho)\\rule{0mm}{8mm}&=& -\\frac{1}{4}\\log \\frac{1}{4} - \\frac{3}{4} \\log \\frac{3}{4}    = 0.81 \\, \\hbox{bits} = \n",
    "  H(X)   \\, . \\nonumber\n",
    "\\end{eqnarray}\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f5e41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Example 2:</b> non-orthogonal states  \n",
    "<br>\n",
    "<br>    \n",
    "Now consider another source from Alice that produces a set of states with identical probabilities $p_i$  \n",
    "<br>\n",
    "<br>    \n",
    "$$\\{ \\ket{\\psi_i}, p_i\\} = \\left\\{\\rule{0mm}{4mm} (\\ket{0}, p_0= 1/4)\\, , \\, (\\ket{+},p_+ = 3/4)\\right\\}$$  \n",
    "<br>    \n",
    "Bob now writes the density matrix  \n",
    "<br>    \n",
    "<br> \n",
    "$$\n",
    "\\rho = \\frac{1}{4}\\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} + \\frac{3}{8} \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 5 & 3 \\\\ 3 & 3 \\end{bmatrix} \\,\n",
    "$$        \n",
    "<br> \n",
    "By diagonalizing, we obtain the eigenvalues  \n",
    "$$\\lambda_i = \\frac{1}{2} \\pm \\frac{1}{4} \\sqrt{\\frac{5}{2}}$$   \n",
    "<br>     \n",
    "Now we compute the Shannon entropy  \n",
    "<br>    \n",
    "$$\n",
    "S(\\rho) = -\\sum_i \\lambda_i \\log \\lambda_i \\, =\\,  0.485\\, \\hbox{bits} \\, <\\,   0.81   \\, \\hbox{bits} ~=~   H(X)\n",
    "$$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2857a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b>  \n",
    "<br><br>\n",
    "The fact that \\( S \\) is smaller than \\( H \\) also suggests that a quantum alphabet \\( X = \\{\\ket{\\psi_a}, p_a\\} \\) could admit an encoding with fewer resources than a classical one. That is, greater compression.  \n",
    "<br><br>\n",
    "The proof of this fact is given by Schumacher’s theorem.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7d833",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measurement Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e461e",
   "metadata": {},
   "source": [
    "Bob receives a system in a state $\\rho$ and applies a projective measurement $\\{E_m = P_m\\}$, where  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P_l^2 = P_l~,~~ P_m P_n = P_m\\delta_{mn}~,~~ \\sum_m P_m = I\n",
    "$$\n",
    "\n",
    "If <u>the outcome is not recorded</u>, the *non-selective measurement* has the following effect on the state:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\rho ~~ \\stackrel{\\{P_m\\}}{\\longrightarrow} ~~ \\rho' = \\sum_m P_m \\rho P_m\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce26a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Theorem:</b> \n",
    "<br>    \n",
    "In a non-selective projective measurement, the entropy does not decrease, \n",
    "$\n",
    "S(\\rho') \\geq S(\\rho)\n",
    "$.    \n",
    "The inequality is saturated when the basis of the projective measurement diagonalizes $\\rho = \\sum_m\\lambda_m P_m $.\n",
    "<br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae065f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary><p > >> <i>Proof</i> </p></summary>\n",
    "    \n",
    "We want to prove that \n",
    "$$\n",
    "0 ~\\leq -S(\\rho) + S(\\rho') = -S(\\rho) -\\tr (\\rho'\\log \\rho')\n",
    "$$\n",
    "    \n",
    "We know a very similar inequality, Klein's inequality, \n",
    "    \n",
    "$$\n",
    "0~\\leq ~S(\\rho\\|\\rho') ~=~ -S(\\rho) -\\tr (\\rho\\log \\rho')\n",
    "$$\n",
    "    \n",
    "It would be sufficient to prove that the second terms are equal: $\\tr (\\rho\\log \\rho') = \\tr (\\rho'\\log \\rho')$\n",
    "\n",
    "$$\n",
    "\\tr (\\rho'\\log \\rho') = \\tr\\left[ \\sum_l  P_l \\rho P_l \\log \\rho' \\right] \n",
    "$$\n",
    "Let's examine: \n",
    "\\begin{eqnarray}\n",
    "P_l \\rho' &=& P_l\\sum_m P_m \\rho P_m = \\sum_m P_l\\delta_{lm}\\rho P_m = P_l\\rho P_l \\nonumber \\\\\n",
    "\\rho'P_l  &=& \\sum_m P_m \\rho P_mP_l = \\sum_m P_m\\rho P_l \\delta_{lm} = P_l\\rho P_l\\nonumber  \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "From this we deduce that $~\\rho' P_l = P_l \\rho' ~\\Rightarrow ~\\log\\rho' P_l = P_l \\log \\rho'~$,\n",
    "and therefore\n",
    "    \n",
    "$$\n",
    "\\tr(\\rho'\\log \\rho') =  \\tr\\left[\\sum_l\\left( P_l \\rho \\log \\rho' P_l\\right)\\right]= \\tr \\left[\\left(\\sum_l P_l^2\\right) \\rho \\log \\rho'\\right] = \\tr(\\rho\\log \\rho')\n",
    "$$\n",
    "\n",
    "and thus we arrive at the desired result.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c6957",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise:</b>  \n",
    "<br> Work in a Hilbert space $\\Hil$ of dimension 6. Randomly define a collective $\\{\\ket{\\psi_a},q_a\\},\\, a = 0,...,r-1$.  \n",
    "Perform a non-selective projective measurement in the computational basis $\\ket{i}$. Obtain the entropy variation due to the measurement.  \n",
    "\n",
    "Repeat the process, performing the measurement in the eigenbasis $\\ket{\\lambda_i}$ of $\\rho$.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5a6e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy of Statistical Mixtures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73a128",
   "metadata": {},
   "source": [
    "The idea is to compare the entropies of a series of states $\\rho_i\\, i=1,2,...r$ with that of a statistical mixture of mixed states\n",
    "\n",
    "$$\n",
    "\\rho = \\sum_{i=1}^r p_i \\rho_i\n",
    "$$\n",
    "with $p_i \\geq 0, \\, \\sum_{i=1}^r p_i = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241f1a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Theorem:</b> \n",
    "<br>    \n",
    "Let $\\rho = \\sum_{i=1}^r p_i\\rho_i$ be a statistical mixture of states $\\rho_i$ with probabilities $p_i\\geq 0,\\, \\sum_{i=1}^r p_i = 1$. The following inequalities hold:\n",
    "<br> \n",
    "<br>    \n",
    "$$\\fbox{$\n",
    "~\\sum_{i=1}^r p_i S(\\rho_i) ~~\\leq~  S(\\rho) ~\\leq~ \\sum_{i=1}^r p_i S(\\rho_i) + H(\\{p_i\\}) ~\n",
    "$}\n",
    "$$   \n",
    "<br>\n",
    "<br>\n",
    "The inequality on the right is saturated when the $\\rho_i$ have support on mutually orthogonal subspaces.    \n",
    "<br>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9097952",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<details>\n",
    "<summary><p > >> <i>Proof</i> </p></summary>\n",
    "\n",
    "The inequality on the left has already been mentioned; it is the property of <b>concavity</b> of the entropy.\n",
    "Its proof is simple using the triangle inequality, which will be proven later.\n",
    "\n",
    "To prove the inequality on the right, we will start by considering the case where $\\rho_i = \\ketbra{\\psi_i}{\\psi_i}$ are pure states, **not necessarily orthogonal**.\n",
    "\n",
    "We introduce an auxiliary system $B$ with dimension $d_B \\geq r$ and orthonormal basis $\\{\\ket{i}\\}$ and define $\\rho_{AB} = \\ketbra{AB}{AB}$ in terms of the entangled state\n",
    "\n",
    "$$\n",
    "\\ket{AB} = \\sum_{i=1}^r \\sqrt{p_i} \\ket{\\psi_i}\\ket{i}\n",
    "$$\n",
    "\n",
    "Since $\\rho_{AB}$ is pure, its partial traces coincide and, therefore, their entropies are also equal:\n",
    "\n",
    "$$\n",
    "S(B) = S(A) = S\\big(\\sum_{i} p_i\\ketbra{\\psi_i}{\\psi_i}\\big) = S(\\rho)\n",
    "$$\n",
    "\n",
    "Next, we perform a projective, non-selective measurement on $B$ using the projectors $P_i^B = \\ketbra{i}{i}$\n",
    "\n",
    "$$\n",
    "\\rho_B'= \\sum_i P_i^B\\rho_B P_i^B = \\sum_i p_i \\ketbra{i}{i}\n",
    "$$\n",
    "\n",
    "We have proven that a non-selective measurement can only increase the entropy, i.e.,\n",
    "\n",
    "$$\n",
    "S(B)= S(\\rho) \\leq S(\\rho_B') = - \\sum_i p_i\\log p_i = H(\\{p_i\\})\n",
    "$$\n",
    "\n",
    "Therefore, when $\\rho_i$ are pure states, we have:\n",
    "\n",
    "$$\n",
    "S (\\rho) \\leq H(\\{p_i\\}) + \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    "\n",
    "where we added the last term, which is zero. The inequality is saturated if the $\\ket{\\psi_i}$ are orthogonal.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Now we can address the general case in which the $\\rho_i$ are mixed states. The spectral decomposition of each $\\rho_i$ is\n",
    "\n",
    "$$\n",
    "\\rho_i = \\sum_{j=1}^N \\pi^i_j \\ketbra{e^i_j}{e^i_j}\n",
    "$$\n",
    "\n",
    "where the $r$ bases $\\{\\ket{e^i_j}\\, , j=1,...,N\\}$ are, in principle, different. We can now write\n",
    "\n",
    "$$\n",
    "\\rho ~=~ \\sum_{i=1}^r \\sum_{j=1}^N p_i \\pi^i_j \\ketbra{e^i_j}{e^i_j}  ~= ~\\sum_{i,j} q_{ij}\\rho_{ij}\n",
    "$$\n",
    "\n",
    "where we have considered $\\rho$ as a mixture of pure states $\\rho_{ij}$. We can apply the result found for pure states to this case:\n",
    "<br>\n",
    "\n",
    "$$\n",
    "S(\\rho) \\leq H(\\{q_{ij}\\}) = -\\sum_{ij} p_i\\pi^i_j \\log(p_i\\pi^i_j) = -\\sum_{ij} p_i\\pi^i_j(\\log p_i + \\log \\pi^i_j)\n",
    "= -\\sum_i p_i \\log p_i -\\sum_i p_i\\big( \\sum_j \\pi^i_j \\log \\pi^i_j \\big)\n",
    "$$\n",
    "\n",
    "where we used that $\\sum_j \\pi^i_j = \\tr(\\rho_i) = 1$. We now recognize in the last two terms the functions $H(\\{p_i\\})$ and $S(\\rho_i)$, that is:\n",
    "\n",
    "$$\n",
    "S(\\rho) \\leq H(\\{p_i\\}) + \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    " \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c19b4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The inequality on the left is the concavity property, whose content is that the entropy of a mixture is greater than that of its parts.\n",
    "\n",
    "The difference is an important quantity that *should be maximized*, as it increases the amount of information in the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d029a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Definition:</b>  <i>(Holevo information)</i>\n",
    "<br>    \n",
    "The <i>Holevo information</i> of a state $\\rho = \\sum_i p_i \\rho_i$ is defined as the entropy increase associated with the statistical mixture \n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "\\chi = S(\\rho) - \\sum_i p_i S(\\rho_i)    \n",
    "$$\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848322c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the previous theorem, by subtracting the quantity $\\rho = \\sum_{i=1}^r p_i \\rho_i$ from all terms, the following inequality for the Holevo information is verified:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Corollary:</b> \n",
    "In a statistical mixture $\\rho = \\sum_{i=1}^r p_i \\rho_i$, the Holevo information is bounded as follows:\n",
    "<br>\n",
    "<br>    \n",
    "$$ \n",
    "0 \\leq \\chi(\\rho) \\leq H(\\{p_i\\}) \n",
    "$$\n",
    "<br>    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160fd67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise:</b> \n",
    "<br> Work in a Hilbert space $\\Hil$ of dimension 6. Randomly define three ensembles $\\{\\ket{\\psi_a},q_a\\}_I$ with $a = 0,...,r_a-1$ and  $I=0,1,2$. With 3 random probabilities $\\{p_i\\},\\, i=0,1,2$, consider the statistical mixture $\\rho = \\sum_i p_i \\rho_i$. Compute the Holevo information and verify the bounds.\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298058be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"ent_comp\"></a>\n",
    "<table width=\"100%\">\n",
    "    <td style=\"font-size:25px;font-family:Helvetica;text-align:left;background-color:rgba(0,0,900, 0.3);\">\n",
    "<b>Quantum Entropies of Composite Systems</b>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952f25a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After the information contained in a state, the next important quantity we wish to understand is the *degree of correlation* between two systems $A$ and $B$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1677062",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Considered jointly, the isolated bipartite system $AB \\sim \\Hil_A \\otimes \\Hil_B$ \n",
    "\n",
    "recall that all accessible information for observers who can measure on $AB$ ($A$, $B$) is contained in $\\rho$, $(\\rho_A, \\rho_B)$\n",
    "\n",
    "in particular, the von Neumann entropy $S(\\rho)$ measures the Shannon uncertainty associated with a preparation via a set of projective measurements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c04e93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entanglement Entropy\n",
    "<a id='ent_entrelaz'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d39ee",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is natural to expect that the degree of entanglement between $A$ and $B$ is reflected in the partial states $\\rho_A$ and $\\rho_B$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6b0c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b> (<i>entanglement entropy</i>)\n",
    "<br>\n",
    "The <b>entanglement entropy</b> is\n",
    "the Von Neumann entropy of a subsystem obtained by taking the partial trace over its complement:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "S(\\rho_A) = \\Tr \\rho_A \\log \\rho_A~~~~~~\\text{with} ~~~~~\\rho_A = \\Tr_B \\rho\n",
    "$$\n",
    "<br>   \n",
    "$$\n",
    "S(\\rho_B) = \\Tr \\rho_B \\log \\rho_B~~~~~~\\text{with} ~~~~~\\rho_B = \\Tr_A \\rho\n",
    "$$  \n",
    "<br>    \n",
    "</div>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4b5af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Notation**: Unless otherwise stated, when dealing with composite systems, we will denote\n",
    "\n",
    "$$ \n",
    "S(AB) = S(\\rho) ~~~~~~~~~ S(A) = S(\\rho_A)  ~~~~~~~~~~ S(B) = S(\\rho_B)\n",
    "$$\n",
    "\n",
    "where it is understood that these refer to the systems obtained via partial traces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8acacbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy of an Uncorrelated State\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f5d48",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Theorem</b> <i></i> \n",
    "<br>\n",
    "for an uncorrelated state we find\n",
    "$$\n",
    "S(\\rho) = S (\\rho_A\\otimes \\rho_B) = S(\\rho_A) + S(\\rho_B)\n",
    "$$\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68153f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "<summary><p style=\"text-align: right ; color:black\"> >> Proof </p></summary>\n",
    "Working with the spectral decompositions of $\\rho_A$ and $\\rho_B$, we find that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log (\\rho_A\\otimes \\rho_B) &=& \\big( \\sum_{i,a} \\log(\\lambda_i \\mu_a) \\ketbra{\\lambda_i \\mu_a}{\\lambda_i\\mu_a}\\big) \\\\\n",
    "&=& \\big( \\sum_{i,a} (\\log\\lambda_i+ \\log \\mu_a)  \\ketbra{\\lambda_i \\mu_a}{\\lambda_i\\mu_a}\\big)  + \\\\\n",
    "&=& \\sum_{i} \\log \\lambda_i \\ketbra{\\lambda_i}{\\lambda_i}\\otimes \\sum_a \\ketbra{\\mu_a}{\\mu_a} + \n",
    "\\sum_{i}  \\ketbra{\\lambda_i}{\\lambda_i}\\otimes \\sum_a \\log \\mu_a \\ketbra{\\mu_a}{\\mu_a} \\\\\n",
    "&=& \\log\\rho_A\\otimes I + I\\otimes \\log \\rho_B\n",
    "\\end{eqnarray}  \n",
    "Then\n",
    "\\begin{eqnarray}\n",
    "S(\\rho_A\\otimes \\rho_B) &=& -\\tr_{AB} \\left[(\\rho_A\\otimes \\rho_B)\\log (\\rho_A\\otimes\\rho_B)\\right] \\\\\n",
    " &=& -\\tr_{AB} \\left[(\\rho_A\\otimes \\rho_B)(\\log\\rho_A\\otimes I + I\\otimes \\log \\rho_B)\\right] \\\\\n",
    "&=&- \\tr_{AB}\\left[\\rho_A\\log\\rho_A\\otimes \\rho_B + \\rho_A \\otimes \\rho_B \\log\\rho_B\\right]\\\\\n",
    "&=& \\tr(\\rho_A\\log\\rho_A)\\otimes \\tr \\rho_B + \\tr\\rho_A \\otimes \\tr(\\rho_B \\log\\rho_B)\\\\\n",
    "&=& S(A) + S(B)\n",
    "\\end{eqnarray}\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377b88e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This notion of non-correlation is analogous to that which exists in classical probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc33950",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Classically, there are several entropies that play a central role: conditional entropy $H(X|Y)$, relative entropy $H(X\\|Y)$, and mutual information $I(X,Y)$.\n",
    "All three admit interpretations in terms of uncertainties and expectations.\n",
    "\n",
    "We can define *formally analogous* quantities in the quantum context, even though the probabilistic interpretation is not as evident—or may even be unknown.\n",
    "\n",
    "We have already defined [relative entropy](#entrop_rel), along with its important property of positivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8531c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id ='infor_mutua'></a>\n",
    "## Mutual Information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82f71f",
   "metadata": {},
   "source": [
    "The definition of mutual information is the same, replacing Shannon entropy with von Neumann entropy  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "I(A,B) = S(A) + S(B) - S(AB) \n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a3499",
   "metadata": {
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem</b> <i>(Positivity of Mutual Information)</i> \n",
    "<br>\n",
    "$$\n",
    "I(A,B) \\geq 0\n",
    "$$\n",
    "<br>    \n",
    "and the inequality is saturated, $I(A,B) = 0$, if and only if $\\rho = \\rho_A\\otimes \\rho_B$ is factorizable.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d5aef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "<summary><p style=\"text-align: right ; color:black\"> >> Demostración </p></summary>\n",
    "<br>\n",
    "<br>    \n",
    "Consideremos la entropía relativa asociada a los estados $\\rho  = \\rho_{AB}$ y  $\\sigma = \\rho_A\\otimes \\rho_B$\n",
    "<br>\n",
    "Entonces, por la desigualdad de Klein\n",
    "<br>    \n",
    "<br>\n",
    "\\begin{eqnarray}\n",
    "0\\leq S(\\rho\\|\\sigma) &=& \\tr\\left(\\rho_{AB}(\\log \\rho_{AB} - \\log (\\rho_A\\otimes \\rho_B) \\rule{0mm}{4mm}\\right)\\nonumber\\\\  \\rule{0mm}{8mm}\n",
    "&=& \\tr \\left(\\rho_{AB}(\\log \\rho_{AB} - \\log (\\rho_A\\otimes I) -\\log ( I \\otimes \\rho_B) \\rule{0mm}{4mm} \\right)\n",
    "\\\\ \\rule{0mm}{8mm}\n",
    "&=& S(AB) - \\tr_{A}(\\tr_B\\rho_{AB}) - \\tr_B(\\tr_A\\rho_{AB}) \\\\ \\rule{0mm}{8mm}\n",
    "&=& S(AB) - S(A) - S(B) \n",
    "\\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "<br>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b90cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Entropy\n",
    "\n",
    "Finally, we will copy the definition of conditional entropy, even though a notion of quantum conditional probability does not exist\n",
    "\n",
    "$$\n",
    "S(A|B) = S(AB) - S(B)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92082b3b",
   "metadata": {},
   "source": [
    "Here we find a genuine feature:\n",
    "unlike the classical case, $S(A|B)$ *can be negative*. \n",
    "\n",
    "- For example, if $AB$ is a pure state $\\Rightarrow S(AB) = 0$\n",
    "<br>\n",
    "\n",
    "- However, it cannot be *too* negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc818b3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b>  \n",
    "<br>\n",
    "$$\n",
    "S(A|B) \\geq - \\hbox{min}(S_A, S_B)\n",
    "$$  \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36432c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "    <summary><p style=\"text-align: right; color:black\"> >> <i>Proof</i> </p></summary>\n",
    "<br>\n",
    "On one hand, from the fact that $S(AB)\\geq 0$ it follows that\n",
    "$$\n",
    "S(A|B) \\geq - S(B)\n",
    "$$ \n",
    "<br>    \n",
    "On the other hand, let us couple $AB$ to a third system $C$ and consider a pure state $ABC$ (i.e., $\\rho_{ABC} = \\ket{\\psi_{ABC}}\\bra{\\psi_{ABC}}$).\n",
    "    \n",
    "Then we know that $S(AB) = S(C)$ and also $S(B) = S(AC)$. We find that\n",
    "<br>    \n",
    "$$\n",
    "S(A|B) = S(AB)- S(B) = S(C) - S(AC) \\geq -S(A)\n",
    "$$\n",
    "<br>    \n",
    "where we have used the positivity of the mutual information between $A$ and $C$.  \n",
    "\n",
    "Thus the result follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d7f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Conditional entropy plays a fundamental role in the possibility of establishing *teleportation protocols*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c4534",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='desig_triang'></a>\n",
    "### Araki–Lieb Triangle Inequality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617ac16",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can now state certain relationships between the entropy of a system $AB$ and that of its parts $A$ and $B$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af613ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem</b> \n",
    "<br>   \n",
    "The von Neumann entropies of a composite system $AB$ and its parts $A,B$ satisfy, for any state $\\rho$, the following inequality:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "|S(A) - S(B)| ~~ \\leq ~~ S(AB) ~~\\leq ~~S(A) + S(B)\n",
    "$$   \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "- The inequality on the left is known as the *Araki–Lieb inequality*.  \n",
    "<br>\n",
    "\n",
    "- The inequality on the right is known as the *subadditivity* of entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45a2da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "    <summary><p style=\"text-align:right ; color:black\"> >> <i>Proof</i> </p></summary>\n",
    "<br>\n",
    "The inequality on the right is called <i>subadditivity</i> and it is equivalent to the positivity of relative entropy.\n",
    "<br>\n",
    "<br>\n",
    "To prove the inequality on the left (Araki–Lieb), we recall the lower bound for conditional entropy:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{eqnarray}\n",
    " S(A|B) = S(AB) - S(B) &\\geq & - \\hbox{min}(S_A,S_B) \\geq -S_A \\\\\n",
    " S(B|A) = S(AB) - S(A) &\\geq& - \\hbox{min}(S_A,S_B) \\geq -S_B \\rule{0mm}{6mm}\n",
    "\\end{eqnarray}  \n",
    "<br>\n",
    "From this it follows that\n",
    "$$\n",
    "S(AB) \\geq  |S(A)- S(B)|\n",
    "$$\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce7056",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The subadditivity property allows for a simple proof of the *concavity* of the von Neumann entropy.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "    <b>Corollary:</b> The von Neumann entropy is a <b>concave</b> function of its argument\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "\\sum_i p_i S(\\rho_i) \\leq S\\left(\\sum_i p_i \\rho_i\\right)\n",
    "$$\n",
    "</p>\n",
    "</div>    \n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right ; color:black\"> >> <i>Proof</i> </p></summary>    \n",
    "\n",
    "Let $\\{\\lambda_{i,a}\\}, a=1,...,d_A$ be the eigenvalues of $\\rho_i$, then\n",
    "$$\n",
    "S(\\rho_i ) = -\\sum_{a}\\lambda_{i,a}\\log\\lambda_{i,a}\n",
    "$$\n",
    "\n",
    "Consider an auxiliary system $B$ with orthonormal basis  $\\{\\ket{i}\\}$ and density matrix\n",
    "$$\n",
    "\\rho_B = \\sum_i p_i \\ket{i}\\bra{i}\n",
    "$$\n",
    "and define the following joint $AB$ state\n",
    "$$\n",
    "\\rho_{AB} = \\sum_{i} p_i \\rho_i \\otimes \\ket{i}\\bra{i}\n",
    "$$\n",
    "If $\\rho_i$ has eigenvectors $\\ket{e_{i,a}}$ with eigenvalues $\\lambda_{i,a}$ such that $\\sum_a \\lambda_{i,a} = 1$, then the eigenvalues of $\\rho_{AB}$ are $\\{ p_i \\lambda_{i,a} \\}$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "S_{AB} &=& -\\sum_{i,a} p_i \\lambda_{i,a} \\log( p_i \\lambda_{i,a})  \\\\\n",
    "&=& \\sum_i p_i \\log p_i \\sum_a \\lambda_{i,a} + \\sum_i p_i \\sum_a \\lambda_{i,a} \\log \\lambda_{i,a} \\nonumber\\\\\n",
    "&=& \\sum_i p_i \\log p_i + \\sum_i p_i S(\\rho_i) \\nonumber\\\\\n",
    "&=& S_B + \\sum_i p_i S(\\rho_i) \n",
    "\\end{eqnarray}\n",
    "\n",
    "Taking the partial traces we find\n",
    "\\begin{eqnarray}\n",
    "\\rho_A &=& \\tr_B \\rho_{AB} =  \\sum_{i} p_i \\rho_i \\nonumber\\\\\n",
    "\\rho_B &=& \\tr_A \\rho_{AB} = \\sum_i p_i \\ket{i}\\bra{i} \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now, using the subadditivity property\n",
    "$$\n",
    "S_{AB} \\leq S_A + S_B\n",
    "$$\n",
    "we obtain \n",
    "$$\n",
    "S_B + \\sum_i p_i S(\\rho_i) \\leq S_A + S_B\n",
    "$$\n",
    "and canceling out $S_B$ we get\n",
    "$$\n",
    "\\sum_i p_i S(\\rho_i) \\leq S_A = S\\left(\\sum_i p_i \\rho_i\\right)\n",
    "$$\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37202c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at cases that saturate these inequalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86ffea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Case 1: Saturation of Subadditivity: Factorizable State ##\n",
    "\n",
    "Suppose the system $AB$ is in a *factorizable* composite state. Then the *mutual information* is zero and, consequently, the *subadditivity* is saturated:\n",
    "\n",
    "$$\n",
    "\\rho_{AB} = \\rho_A \\otimes \\rho_B ~~~~\\Leftrightarrow ~~~~ S(AB) = S(A) + S(B)\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab1e4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 2: Saturation of Araki–Lieb: Pure or Entangled State\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836fe84",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We begin by writing the pure state\n",
    "\n",
    "$$\n",
    "\\rho_{AB} = \\ket{\\psi_{AB}} \\bra{\\psi_{AB}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f2a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a pure state of a bipartite system $AB$, entanglement introduces quantum correlations between the two systems\n",
    "\n",
    "$$\n",
    "\\ket{\\psi_{AB}} = \\sum_{i,j} c_{ij} \\ket{i}_A \\otimes \\ket{j}_B\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2956340",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One way to determine if entanglement exists is to use the Schmidt decomposition.\n",
    "\n",
    "$$\n",
    "\\ket{\\psi_{AB}} = \\sum_{a=1}^r \\sqrt{p_a}\\,  \\ket{\\psi^a_A} \\otimes \\ket{\\psi^a_B}\n",
    "$$\n",
    "\n",
    "If and only if the Schmidt number, $r$, is greater than one, the state is entangled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19af94e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we want to be more precise and propose a way to quantify the extent of such correlations. This is what the *Entanglement Entropy* measures.\n",
    "\n",
    "Indeed,\n",
    "\n",
    "$$\n",
    "S(\\rho_A) = S(\\rho_B) = - \\sum_{a=1}^r p_a \\log p_a\n",
    "$$\n",
    "\n",
    "Therefore, the *entanglement entropy* is proportional to the *entanglement* present in $\\ket{\\psi_{AB}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ff8f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b> Let ${AB}$ be a composite system in a pure state with $ S(AB) = 0$. The entanglement entropy of its constituent subsystems $A$ and $B$ is  \n",
    "<br>\n",
    "<br>    \n",
    "$\\to ~$ equal for both subsystems $S(A) = S(B)$\n",
    "<br>\n",
    "<br>    \n",
    "$\\to ~$ proportional to the entanglement of the pure state $\\ket{\\psi_{AB}}$\n",
    "</p>\n",
    "</div>    \n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align: right ; color:black\"> >> Proof </p></summary>\n",
    "<br>\n",
    "The density matrix $\\rho_{AB} = \\ket{\\psi_{AB}}\\bra{\\psi_{AB}}$ has zero entropy $S(\\rho_{AB})=0$. This is not the case for the density matrices of the subsystems $A$ and $B$.\n",
    "Writing $\\ket{\\psi_{AB}}$ in the Schmidt basis, we can compute\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\rho_{A} = \\Tr_B \\rho_{AB} = \\sum_{a} p_a \\ket{\\psi^a_A}\\bra{\\psi^a_A}\n",
    "~~~~~~~,~~~~~~~~\n",
    "\\rho_{B} = \\Tr_A \\rho_{AB} = \\sum_{a} p_a \\ket{\\psi^a_B}\\bra{\\psi^a_B}\n",
    "$$\n",
    "Since the $\\ket{\\psi^a}$ are orthonormal, for the entropies of the subsystems we find\n",
    "$$\n",
    "S(A) = S(B) = - \\sum_{a=1}^r p_a \\log p_a\n",
    "$$\n",
    "<br>\n",
    " - they are proportional to the degree of entanglement of $\\ket{\\psi_{AB}}$. \n",
    "    \n",
    "Indeed, if $p_1= 1$ and $p_{i>1} = 0$, so that there is no entanglement, then the entropies satisfy $S(A) = S(B) = 0$. \n",
    "    \n",
    "On the other hand, if the state is maximally mixed $S = \\log N$, then the entanglement is also maximal with $p_a = \\frac{1}{N}$ for $a = 1,..., N$.\n",
    "<br>\n",
    "<br>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4156ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see that, in this case, the Araki–Lieb inequality is saturated:\n",
    "\n",
    "$$\n",
    "|S(A) - S(B)| = 0 = S(AB)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc1b30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"codif_optim\"></a>\n",
    "<table width=\"100%\">\n",
    "    <td style=\"font-size:25px;font-family:Helvetica;text-align:left;background-color:rgba(0,0,900, 0.3);\">\n",
    "<b>Quantum Coding</b>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca15b71",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "    <b>Theorem:</b> <i>Schumacher's theorem</i>\n",
    "    <br>\n",
    "Given a message whose letters are pure states drawn independently from the quantum alphabet $X=  \\{ \\ket{\\psi_i}, p_i\\}$,  there exists \n",
    "    an <i>optimal lossless coding</i> that makes an average use of $S(\\rho)$ <i>qubits per letter</i>, where $\\rho = \\sum_i p_i \\ketbra{\\psi_i}{\\psi_i}$.\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f254598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
